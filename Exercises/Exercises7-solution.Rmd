---
title: "Exercises 7"
author: "Yongqi WANG, Hangjia ZHAO"
output:
  pdf_document: default
header-includes:
 - \usepackage{amsmath}
 - \usepackage{blkarray} # use blkarray in latex env
 - \usepackage{bm} # use blkarray in latex env
 # - \usepackage{derivative} # use blkarray in latex env
---

\newcommand{\od}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}


## Problem 1

### a)

The derivative of x, y are zero at equilibrium/fixed point:

\begin{align}
0 &= x(a - by) \\
0 &= y(-c + dx)
\end{align}

It's easy to see that: $(0, 0)$ and $(\frac{c}{d}, \frac{a}{b})$ are the fixed points.

### b)

The Jacobian of the RHS:

\begin{align}
J = 
  \begin{bmatrix}
  a - by & -bx \\
  dy & -c + dx
  \end{bmatrix}
\end{align}

For the non-trivial fixed points:

\begin{align}
J = 
  \begin{bmatrix}
  a - b\frac{a}{b} & -b\frac{c}{d} \\
  d\frac{a}{b} & -c + d\frac{c}{d}
  \end{bmatrix}
  =
  \begin{bmatrix}
  0 & \frac{-bc}{d} \\
  \frac{ad}{b} & 0
  \end{bmatrix} 
\end{align}

Eigenvalues can be calculated easily: $\lambda_1 = i\sqrt{ac}, \lambda_2 = -i\sqrt{ac}$. As shown by the eigenvalues which both have a zero real part. This indicates that the equiblirum is not attriactive and is not repulsive.

Due to the fact that the eigenvalues have a non-zero imaginary part, the system will now oscillate awith a period of $\sqrt{ac}$

### c)

When solving this question, we consulted [this lecture note](http://www.math.harvard.edu/library/sternberg/slides/11809LV.pdf) as well as some other books.

Consider the replicator equation in $n$ variables:
\begin{align}
\dot{x_i} &= x_i \left(r_i + \sum_j a_{ij}x_j \right), \quad i = 1, \dots, n+1
\end{align}

This can be re-written into:

\begin{align}
\dot{x_i} &= x_i \left((\bm{Ax})_i - \bm{x}\cdot \bm{Ax}\right), \quad i = 1, \dots, n+1
\end{align}

We have $x_i$ denoting the density of $i$-th species and $r_i$ denoting the $i$-th species growth rate. Let $a_{ij}$ denote the interaction between $i$-th and $j$-th and the matrix $\bm{A}$ with entry $a_{ij}$ be the interaction matrix. With the last row of $\bm{A}$ being zeros.

Define $y_{n+1} = 1$ and define a mapping from $\bm{y}$ to $\bm{x}$ given by:

\begin{align}
y_i = \dfrac{y_i}{\sum_{j=1}^n y_j}, \quad i = 1, \dots, n+1
\end{align}

The inverse mapping from $\bm{x}$ to $\bm{y}$ given by:

\begin{align}
x_i = \dfrac{x_i}{x_n}, \quad i = 1, \dots, n+1
\end{align}

Since $(\bm{Ax})_n = 0$ and the ODE systems on $x_i$ above. we have:

\begin{align}
\dot{y_i} &= \left(\dfrac{x_i}{x_n}\right)\cdot \left((\bm{Ax})_i - (\bm{Ax})_n\right) \\
&= y_i \left(\sum a_{ij}x_j\right) \\
&= y_i \left(\sum_{j=1}^{n+1} a_{ij}y_i\right) x_n \quad \text{Since } x_j = y_j\cdot x_n \\
&= y_i (a_{in} + \sum_{j=1}^n a_{ij} y_j) \quad x_n \text{removed}
\end{align}


## Problem 2

### a)

A matrix is called a stochastics matrix if
\begin{enumerate}
  \item it is a square matrix
  \item $0\leq A_{ij} \leq 1, \quad \forall\, i, j$
  \item $\sum_j A_{ij} = 1, \quad \forall \, i,j$
\end{enumerate}

(1) and (2) is trivial since transition can be made from any state to another and hence the square matrix. As $p, q, 1-p, 1-p \in [0, 1]$, (2) is also fulfilled.

By simple calculation, it is not hard to see that the row of matrix $M$ equals to 1.

### b)

To find the stationary distribution of the transition, let $x_t$ be the distribution after $t$ transition.

If $x$ stated in the question were the stationary distribution, 

\begin{align}
\lim_{t\to\infty} x_t \cdot M = x
\end{align}

To verify (just the first component of $x_t$ for simplicity, denoted by $x_t^{1}$)
Provided that $x_t = (s_1s_2, s_1(1-s_2), (1-s_1)s_2, (1-s_1)(1-s_2))$
\begin{align}
x_{t+1}^{1} &= x_t^1{} \cdot M \\
&= s_1s_2 \cdot p_1p_2 + s_1(1-s_2) \cdot q_1p_2 + (1-s_1)s_2 \cdot p_1q_2 + (1-s_1)(1-s_2) \cdot q_1q_2 \\
&= s_1s_2 \bigg(p_1p_2 - q_1p_2 +p_1q_2 + q_1q_2 \bigg) s_1q_1p_2 + s_2p_1q_2 - (s_1 + s_2)q_1q_2 + q_1q_2 \\
&= s_1s_2r_1r_2 + s_1q_1r_2 + s_2q_2r_1 + q_1q_2 \\
&= \Bigg{[} \Big( (q_2r_1+q_1)(q_1r_2+q_2)r_1r_2 \Big) + \Big( (q_2r_1+q_1)r_2q_1(1-r_1r_2) \Big) \\
&  \quad + \Big( (q_1r_2 + q_2)r_1q_2 (1-r_1r_2) \Big) + \Big( (1-r_1r_2)^2q_1q_2\Big) \Bigg{]} \cdot \frac{1}{(1-r_1r_2)^2}  \\
&= \Bigg{[} \Big(q_2^2r_1^2r_2^2 + q_2^2r_1^2r_2 + q_1q_2r_2^2r_1 + q_1q_2r_1r_2\Big) + \Big(q_1q_2r_1r_2 + q_1^2r_2 - q_1q_2r_1^2r_2^2\Big) \\
&  \quad + \Big(q_1q_2r_1r_2 + q_2^2r_1 - q_1q_2r_1^2r_2^2 - q_2^2r_1^2r_2\Big) + \Big(q_1q_2 - 2q_1q_2r_1r_2 + q_1q_2r_1^2r_2^2\Big) \Bigg{]} \cdot \frac{1}{(1-r_1r_2)^2} \\
&= \frac{q_1q_2r_1r_2 +q_1^2r_2 +q_1q_2+q_2^2r_1}{(1-r_1r_2)^2} \\
&= s_1s_2
\end{align}

### c)

It is easy to see this strategy is tit-for-tat. From the results from (b), we can show the expected payoffs for both players with the help of $s_1, s_2$

In the setting of strategy $S_1(1, 0)$, 

\begin{align}
p_1 = 1 \\
q_1 = 0
\end{align}

\begin{align}
s_1 = \frac{q_2}{1 + q_2 - p_2} \\
s_2 = \frac{q_2}{1 + q_2 - p_2}
\end{align}

It is easy to see that $s_1 = s_2$ and hence the expected payoff at the stationary distribution of the correspounding Markov chain is the same for both players.

### d)

To calculate the expected payoff for $S_1$ against $S_2$, calculate $s_1$ and $s_2$,

\begin{align}
s_1 &= \frac{\frac{1}{4}(1-0) + 0}{1 - (1-0)(1-\frac{1}{4})} = 1\\
s_2 &= \frac{0(1-\frac{1}{4}) + \frac{1}{4}}{1 - (1-0)(1-\frac{1}{4})} = 1
\end{align}

The expected paoff at the stationary distribution:

$$
E(S_1, S_2) = Rs_1s_2 + Ss_1(1-s_2) + T(1-s_1)s_2 + P(1-s_1)(1-s_2) = 3
$$

The expected payoff for this game with $S_1, S_2$ is $3$.

Code is available on github repo:
(https://github.com/wyq977/evolutionary-dynamics-2019)
